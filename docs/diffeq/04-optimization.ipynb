{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimization Problems\n",
    "\n",
    "From: https://mtk.sciml.ai/dev/tutorials/optimization/\n",
    "\n",
    "## 2D Rosenbrock Function\n",
    "\n",
    "Wikipedia: https://en.wikipedia.org/wiki/Rosenbrock_function\n",
    "\n",
    "Find $(x, y)$ that minimizes the loss function $(a - x)^2 + b(y - x^2)^2$"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ModelingToolkit\n",
    "using Optimization\n",
    "using OptimizationOptimJL"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@variables begin\n",
    "    x, [bounds = (-2.0, 2.0)]\n",
    "    y, [bounds = (-1.0, 3.0)]\n",
    "end\n",
    "\n",
    "@parameters a = 1 b = 1"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Target (loss) function"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss = (a - x)^2 + b * (y - x^2)^2"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The OptimizationSystem"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@named sys = OptimizationSystem(loss, [x, y], [a, b])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "MTK can generate Gradient and Hessian to solve the problem more efficiently."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "u0 = [\n",
    "    x => 1.0\n",
    "    y => 2.0\n",
    "]\n",
    "p = [\n",
    "    a => 1.0\n",
    "    b => 100.0\n",
    "]\n",
    "\n",
    "prob = OptimizationProblem(sys, u0, p, grad=true, hess=true)\n",
    "\n",
    "# The true solution is (1.0, 1.0)\n",
    "sol = solve(prob, GradientDescent())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding constraints\n",
    "`OptimizationSystem(..., constraints = cons)`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@variables begin\n",
    "    x, [bounds = (-2.0, 2.0)]\n",
    "    y, [bounds = (-1.0, 3.0)]\n",
    "end\n",
    "\n",
    "@parameters a = 1 b = 100\n",
    "\n",
    "loss = (a - x)^2 + b * (y - x^2)^2\n",
    "cons = [\n",
    "    x^2 + y^2 â‰² 1,\n",
    "]\n",
    "\n",
    "@named sys = OptimizationSystem(loss, [x, y], [a, b], constraints=cons)\n",
    "\n",
    "u0 = [x => 0.14, y => 0.14]\n",
    "prob = OptimizationProblem(sys, u0, grad=true, hess=true, cons_j=true, cons_h=true)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use interior point Newton method for contrained optimization"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solve(prob, IPNewton())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameter estimation\n",
    "\n",
    "From: https://docs.sciml.ai/DiffEqParamEstim/stable/getting_started/\n",
    "\n",
    "`DiffEqParamEstim.jl` is not installed with `DifferentialEquations.jl`. You need to install it manually:\n",
    "\n",
    "```julia\n",
    "using Pkg\n",
    "Pkg.add(\"DiffEqParamEstim\")\n",
    "using DiffEqParamEstim\n",
    "```\n",
    "\n",
    "The key function is `DiffEqParamEstim.build_loss_objective()`, which builds a loss (objective) function for the problem against the data. Then we can use optimization packages to solve the problem.\n",
    "\n",
    "### Estimate a single parameter from the data and the ODE model\n",
    "\n",
    "Let's optimize the parameters of the Lotka-Volterra equation."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using DifferentialEquations\n",
    "using Plots\n",
    "using DiffEqParamEstim\n",
    "using ForwardDiff\n",
    "using Optimization\n",
    "using OptimizationOptimJL\n",
    "\n",
    "# Example model\n",
    "function lotka_volterra!(du, u, p, t)\n",
    "    du[1] = dx = p[1] * u[1] - u[1] * u[2]\n",
    "    du[2] = dy = -3 * u[2] + u[1] * u[2]\n",
    "end\n",
    "\n",
    "u0 = [1.0; 1.0]\n",
    "tspan = (0.0, 10.0)\n",
    "p = [1.5] ## The true parameter value\n",
    "prob = ODEProblem(lotka_volterra!, u0, tspan, p)\n",
    "sol = solve(prob, Tsit5())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a sample dataset with some noise."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ts = range(tspan[begin], tspan[end], 200)\n",
    "data = [sol.(ts, idxs=1) sol.(ts, idxs=2)] .* (1 .+ 0.03 .* randn(length(ts), 2))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting the sample dataset and the true solution."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(sol)\n",
    "scatter!(ts, data, label=[\"u1 data\" \"u2 data\"])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`DiffEqParamEstim.build_loss_objective()` builds a loss function for the ODE problem for the data.\n",
    "\n",
    "We will minimize the mean squared error using `L2Loss()`.\n",
    "\n",
    "Note that\n",
    "- the data should be transposed.\n",
    "- Uses `AutoForwardDiff()` as the automatic differentiation (AD) method since the number of parameters plus states is small (<100). For larger problems, one can use `Optimization.AutoZygote()`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "alg = Tsit5()\n",
    "\n",
    "cost_function = build_loss_objective(\n",
    "    prob, alg,\n",
    "    L2Loss(collect(ts), transpose(data)),\n",
    "    Optimization.AutoForwardDiff(),\n",
    "    maxiters=10000, verbose=false\n",
    ")\n",
    "\n",
    "plot(\n",
    "    cost_function, 0.0, 10.0,\n",
    "    linewidth=3, label=false, yscale=:log10,\n",
    "    xaxis=\"Parameter\", yaxis=\"Cost\", title=\"1-Parameter Cost Function\"\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is a dip (minimum) in the cost function at the true parameter value (1.5). We can use an optimizer, e.g., `Optimization.jl`, to find the parameter value that minimizes the cost. (1.5 in this case)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "optprob = Optimization.OptimizationProblem(cost_function, [1.42])\n",
    "optsol = solve(optprob, BFGS())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The fitting result:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "newprob = remake(prob, p=optsol.u)\n",
    "newsol = solve(newprob, Tsit5())\n",
    "plot(sol)\n",
    "plot!(newsol)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Estimate multiple parameters\n",
    "Let's use the Lotka-Volterra (Fox-rabbit) equations with all 4 parameters free."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function f2(du, u, p, t)\n",
    "    du[1] = dx = p[1] * u[1] - p[2] * u[1] * u[2]\n",
    "    du[2] = dy = -p[3] * u[2] + p[4] * u[1] * u[2]\n",
    "end\n",
    "\n",
    "u0 = [1.0; 1.0]\n",
    "tspan = (0.0, 10.0)\n",
    "p = [1.5, 1.0, 3.0, 1.0]  ## True parameters\n",
    "alg = Tsit5()\n",
    "prob = ODEProblem(f2, u0, tspan, p)\n",
    "sol = solve(prob, alg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ts = range(tspan[begin], tspan[end], 200)\n",
    "data = [sol.(ts, idxs=1) sol.(ts, idxs=2)] .* (1 .+ 0.01 .* randn(length(ts), 2))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can find multiple parameters at once using the same steps. True parameters are `[1.5, 1.0, 3.0, 1.0]`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cost_function = build_loss_objective(\n",
    "    prob, alg, L2Loss(collect(ts), transpose(data)),\n",
    "    Optimization.AutoForwardDiff(),\n",
    "    maxiters=10000, verbose=false\n",
    ")\n",
    "optprob = Optimization.OptimizationProblem(cost_function, [1.3, 0.8, 2.8, 1.2])\n",
    "result_bfgs = solve(optprob, BFGS())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  },
  "kernelspec": {
   "name": "julia-1.10",
   "display_name": "Julia 1.10.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
