{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and save DataFrames\n",
    "\n",
    "We do not cover all features of the packages. Please refer to their documentation to learn them.\n",
    "\n",
    "- https://github.com/ExpandingMan/Arrow.jl\n",
    "- https://github.com/invenia/JLSO.jl\n",
    "- https://github.com/JuliaData/JSONTables.jl\n",
    "- https://github.com/xiaodaigh/JDF.jl\n",
    "\n",
    "Here we'll load `CSV.jl` to read and write CSV files and `Arrow.jl`, `JLSO.jl`, and serialization, which allow us to work with a binary format and `JSONTables.jl` for JSON interaction. Finally we consider a custom `JDF.jl` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using Arrow\n",
    "using CSV\n",
    "using Serialization\n",
    "using JLSO\n",
    "using JSONTables\n",
    "using CodecZlib\n",
    "using ZipFile\n",
    "using JDF\n",
    "using StatsPlots ## for charts\n",
    "using Mmap ## for compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple `DataFrame` for testing purposes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = DataFrame(\n",
    "    A=[true, false, true], B=[1, 2, missing],\n",
    "    C=[missing, \"b\", \"c\"], D=['a', missing, 'c']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use `eltypes` to look at the columnwise types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV.jl\n",
    "Let's use `CSV` to save `x` to disk; make sure `x1.csv` does not conflict with some file in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"x1.csv\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how it was saved by reading `x.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read(\"x1.csv\", String))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = CSV.read(\"x1.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when loading in a `DataFrame` from a `CSV` the column type for columns `:C` `:D` have changed to use special strings defined in the InlineStrings.jl package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization by JDF.jl and JLSO.jl\n",
    "\n",
    "Now we use serialization to save `x`.\n",
    "\n",
    "There are two ways to perform serialization. The first way is to use the `Serialization.serialize` as below:\n",
    "\n",
    "Note that in general, this process _will not work_ if the reading and writing are done by different versions of Julia, or an instance of Julia with a different system image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"x.bin\", \"w\") do io\n",
    "    serialize(io, x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load back the saved file to `y` variable. Again `y` is identical to `x`. However, please beware that if you session does not have DataFrames.jl loaded, then it may not recognise the content as DataFrames.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = open(deserialize, \"x.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JDF.jl\n",
    "\n",
    "`JDF.jl` is a relatively new package designed to serialize DataFrames. You can save a DataFrame with the `savejdf` function.\n",
    "\n",
    "For more details about design assumptions and limitations of `JDF.jl` please check out https://github.com/xiaodaigh/JDF.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JDF.save(\"x.jdf\", x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the saved JDF file, one can use the `loadjdf` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loaded = JDF.load(\"x.jdf\") |> DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isequal(x_loaded, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JDF.jl offers the ability to load only certain columns from disk to help with working with large files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a JDFFile which is a on disk representation of `x` backed by JDF.jl\n",
    "x_ondisk = jdf\"x.jdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all the names of `x` without loading it into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(x_ondisk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is an example of how to load only columns `:A` and `:D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = JDF.load(x_ondisk; cols=[\"A\", \"D\"]) |> DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JLSO.jl\n",
    "Another way to perform serialization is by using the [JLSO.jl](https://github.com/invenia/JLSO.jl) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JLSO.save(\"x.jlso\", :data => x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load back the file to `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = JLSO.load(\"x.jlso\")[:data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSONTables.jl\n",
    "Often you might need to read and write data stored in JSON format. JSONTables.jl provides a way to process them in row-oriented or column-oriented layout. We present both options below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(io -> arraytable(io, x), \"x1.json\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(io -> objecttable(io, x), \"x2.json\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read(\"x1.json\", String))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read(\"x2.json\", String))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = open(jsontable, \"x1.json\") |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = open(jsontable, \"x2.json\") |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrow.jl\n",
    "Finally we use Apache Arrow format that allows, in particular, for data interchange with R or Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arrow.write(\"x.arrow\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Arrow.Table(\"x.arrow\") |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltype.(eachcol(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that columns of `y` are immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try\n",
    "    y.A[1] = false\n",
    "catch e\n",
    "    show(e)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because `Arrow.Table` uses memory mapping and thus uses a custom vector types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get standard Julia Base vectors by copying a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = copy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2.B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic bechmarking\n",
    "\n",
    "Next, we'll create some files, so be careful that you don't already have these files in your working directory!\n",
    "\n",
    "In particular, we'll time how long it takes us to write a `DataFrame` with 1000 rows and 100000 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdf = DataFrame(rand(Bool, 10^4, 1000), :auto)\n",
    "\n",
    "bigdf[!, 1] = Int.(bigdf[!, 1])\n",
    "bigdf[!, 2] = bigdf[!, 2] .+ 0.5\n",
    "bigdf[!, 3] = string.(bigdf[!, 3], \", as string\")\n",
    "\n",
    "println(\"First run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"CSV.jl\")\n",
    "csvwrite1 = @elapsed @time CSV.write(\"bigdf1.csv\", bigdf)\n",
    "println(\"Serialization\")\n",
    "serializewrite1 = @elapsed @time open(io -> serialize(io, bigdf), \"bigdf.bin\", \"w\")\n",
    "println(\"JDF.jl\")\n",
    "jdfwrite1 = @elapsed @time JDF.save(\"bigdf.jdf\", bigdf)\n",
    "println(\"JLSO.jl\")\n",
    "jlsowrite1 = @elapsed @time JLSO.save(\"bigdf.jlso\", :data => bigdf)\n",
    "println(\"Arrow.jl\")\n",
    "arrowwrite1 = @elapsed @time Arrow.write(\"bigdf.arrow\", bigdf)\n",
    "println(\"JSONTables.jl arraytable\")\n",
    "jsontablesawrite1 = @elapsed @time open(io -> arraytable(io, bigdf), \"bigdf1.json\", \"w\")\n",
    "println(\"JSONTables.jl objecttable\")\n",
    "jsontablesowrite1 = @elapsed @time open(io -> objecttable(io, bigdf), \"bigdf2.json\", \"w\")\n",
    "println(\"Second run\")\n",
    "println(\"CSV.jl\")\n",
    "csvwrite2 = @elapsed @time CSV.write(\"bigdf1.csv\", bigdf)\n",
    "println(\"Serialization\")\n",
    "serializewrite2 = @elapsed @time open(io -> serialize(io, bigdf), \"bigdf.bin\", \"w\")\n",
    "println(\"JDF.jl\")\n",
    "jdfwrite2 = @elapsed @time JDF.save(\"bigdf.jdf\", bigdf)\n",
    "println(\"JLSO.jl\")\n",
    "jlsowrite2 = @elapsed @time JLSO.save(\"bigdf.jlso\", :data => bigdf)\n",
    "println(\"Arrow.jl\")\n",
    "arrowwrite2 = @elapsed @time Arrow.write(\"bigdf.arrow\", bigdf)\n",
    "println(\"JSONTables.jl arraytable\")\n",
    "jsontablesawrite2 = @elapsed @time open(io -> arraytable(io, bigdf), \"bigdf1.json\", \"w\")\n",
    "println(\"JSONTables.jl objecttable\")\n",
    "jsontablesowrite2 = @elapsed @time open(io -> objecttable(io, bigdf), \"bigdf2.json\", \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude JSONTables.jl arraytable due to timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedbar(\n",
    "    repeat([\"CSV.jl\", \"Serialization\", \"JDF.jl\", \"JLSO.jl\", \"Arrow.jl\", \"JSONTables.jl\\nobjecttable\"],\n",
    "        inner=2),\n",
    "    [csvwrite1, csvwrite2, serializewrite1, serializewrite1, jdfwrite1, jdfwrite2,\n",
    "        jlsowrite1, jlsowrite2, arrowwrite1, arrowwrite2, jsontablesowrite2, jsontablesowrite2],\n",
    "    group=repeat([\"1st\", \"2nd\"], outer=6),\n",
    "    ylab=\"Second\",\n",
    "    title=\"Write Performance\\nDataFrame: bigdf\\nSize: $(size(bigdf))\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [\"bigdf1.csv\", \"bigdf.bin\", \"bigdf.arrow\", \"bigdf1.json\", \"bigdf2.json\"]\n",
    "df = DataFrame(file=data_files, size=getfield.(stat.(data_files), :size))\n",
    "append!(df, DataFrame(file=\"bigdf.jdf\", size=reduce((x, y) -> x + y.size,\n",
    "    stat.(joinpath.(\"bigdf.jdf\", readdir(\"bigdf.jdf\"))),\n",
    "    init=0)))\n",
    "sort!(df, :size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@df df plot(:file, :size / 1024^2, seriestype=:bar, title=\"Format File Size (MB)\", label=\"Size\", ylab=\"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"First run\")\n",
    "println(\"CSV.jl\")\n",
    "csvread1 = @elapsed @time CSV.read(\"bigdf1.csv\", DataFrame)\n",
    "println(\"Serialization\")\n",
    "serializeread1 = @elapsed @time open(deserialize, \"bigdf.bin\")\n",
    "println(\"JDF.jl\")\n",
    "jdfread1 = @elapsed @time JDF.load(\"bigdf.jdf\") |> DataFrame\n",
    "println(\"JLSO.jl\")\n",
    "jlsoread1 = @elapsed @time JLSO.load(\"bigdf.jlso\")\n",
    "println(\"Arrow.jl\")\n",
    "arrowread1 = @elapsed @time df_tmp = Arrow.Table(\"bigdf.arrow\") |> DataFrame\n",
    "arrowread1copy = @elapsed @time copy(df_tmp)\n",
    "println(\"JSONTables.jl arraytable\")\n",
    "jsontablesaread1 = @elapsed @time open(jsontable, \"bigdf1.json\")\n",
    "println(\"JSONTables.jl objecttable\")\n",
    "jsontablesoread1 = @elapsed @time open(jsontable, \"bigdf2.json\")\n",
    "println(\"Second run\")\n",
    "csvread2 = @elapsed @time CSV.read(\"bigdf1.csv\", DataFrame)\n",
    "println(\"Serialization\")\n",
    "serializeread2 = @elapsed @time open(deserialize, \"bigdf.bin\")\n",
    "println(\"JDF.jl\")\n",
    "jdfread2 = @elapsed @time JDF.load(\"bigdf.jdf\") |> DataFrame\n",
    "println(\"JLSO.jl\")\n",
    "jlsoread2 = @elapsed @time JLSO.load(\"bigdf.jlso\")\n",
    "println(\"Arrow.jl\")\n",
    "arrowread2 = @elapsed @time df_tmp = Arrow.Table(\"bigdf.arrow\") |> DataFrame\n",
    "arrowread2copy = @elapsed @time copy(df_tmp)\n",
    "println(\"JSONTables.jl arraytable\")\n",
    "jsontablesaread2 = @elapsed @time open(jsontable, \"bigdf1.json\")\n",
    "println(\"JSONTables.jl objecttable\")\n",
    "jsontablesoread2 = @elapsed @time open(jsontable, \"bigdf2.json\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude JSON arraytable due to much longer timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedbar(\n",
    "    repeat([\"CSV.jl\", \"Serialization\", \"JDF.jl\", \"JLSO.jl\", \"Arrow.jl\", \"Arrow.jl\\ncopy\", #\"JSON\\narraytable\",\n",
    "            \"JSON\\nobjecttable\"], inner=2),\n",
    "    [csvread1, csvread2, serializeread1, serializeread2, jdfread1, jdfread2, jlsoread1, jlsoread2,\n",
    "        arrowread1, arrowread2, arrowread1 + arrowread1copy, arrowread2 + arrowread2copy,\n",
    "        # jsontablesaread1, jsontablesaread2,\n",
    "        jsontablesoread1, jsontablesoread2],\n",
    "    group=repeat([\"1st\", \"2nd\"], outer=7),\n",
    "    ylab=\"Second\",\n",
    "    title=\"Read Performance\\nDataFrame: bigdf\\nSize: $(size(bigdf))\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using gzip compression\n",
    "A common user requirement is to be able to load and save CSV that are compressed using gzip. Below we show how this can be accomplished using `CodecZlib.jl`. The same pattern is applicable to `JSONTables.jl` compression/decompression.\n",
    "\n",
    "Again make sure that you do not have file named `df_compress_test.csv.gz` in your working directory.\n",
    "\n",
    "We first generate a random data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(rand(1:10, 10, 1000), :auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GzipCompressorStream comes from CodecZlib\n",
    "open(\"df_compress_test.csv.gz\", \"w\") do io\n",
    "    stream = GzipCompressorStream(io)\n",
    "    CSV.write(stream, df)\n",
    "    close(stream)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = CSV.File(transcode(GzipDecompressor, Mmap.mmap(\"df_compress_test.csv.gz\"))) |> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df == df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using zip files\n",
    "\n",
    "Sometimes you may have files compressed inside a zip file.\n",
    "\n",
    "In such a situation you may use [ZipFile.jl](https://github.com/fhs/ZipFile.jl) in conjunction an an appropriate reader to read the files.\n",
    "\n",
    "Here we first create a ZIP file and then read back its contents into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = DataFrame(rand(1:10, 3, 4), :auto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = DataFrame(rand(1:10, 3, 4), :auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we show yet another way to write a `DataFrame` into a CSV file:\n",
    "writing a CSV file into the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ZipFile.Writer(\"x.zip\")\n",
    "\n",
    "f1 = ZipFile.addfile(w, \"x1.csv\")\n",
    "write(f1, sprint(show, \"text/csv\", df1))\n",
    "\n",
    "# write a second CSV file into zip file\n",
    "f2 = ZipFile.addfile(w, \"x2.csv\", method=ZipFile.Deflate)\n",
    "write(f2, sprint(show, \"text/csv\", df2))\n",
    "\n",
    "close(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the CSV we have written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ZipFile.Reader(\"x.zip\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index index of file called x1.csv\n",
    "index_xcsv = findfirst(x -> x.name == \"x1.csv\", z.files)\n",
    "# to read the x1.csv file in the zip file\n",
    "df1_2 = CSV.read(read(z.files[index_xcsv]), DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_2 == df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index index of file called x2.csv\n",
    "index_xcsv = findfirst(x -> x.name == \"x2.csv\", z.files)\n",
    "# to read the x2.csv file in the zip file\n",
    "df2_2 = CSV.read(read(z.files[index_xcsv]), DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_2 == df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that once you read a given file from `z` object its stream is all used-up (it is at its end). Therefore to read it again you need to close `z` and open it again.\n",
    "\n",
    "Also do not forget to close the zip file once done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
